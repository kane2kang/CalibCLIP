# CalibCLIP Base Configuration

# Model settings
model:
  name: "ViT-B/16"
  image_size: [384, 128]  # [H, W]
  stride_size: 16
  embed_dim: 512
  pretrained_path: null

  # Vision encoder
  vision:
    width: 768
    layers: 12
    patch_size: 16

  # Text encoder
  text:
    context_length: 77
    vocab_size: 49408
    width: 512
    heads: 8
    layers: 12

# CVE settings
cve:
  enabled: true
  residual_coefficient: 0.1

# DCC settings
dcc:
  enabled: true
  attention_threshold: null  # Auto-compute based on data
  lambda_weight: 0.5
  top_k: 100

# Data settings
data:
  dataset: "cuhkpedes"
  root: "./data/CUHK-PEDES"
  json_path: null
  split: "test"
  context_length: 77
  num_workers: 4

  # Normalization (CLIP default)
  mean: [0.48145466, 0.4578275, 0.40821073]
  std: [0.26862954, 0.26130258, 0.27577711]

# Evaluation settings
eval:
  batch_size: 128
  num_workers: 4
  topk: [1, 5, 10]
  compute_map: true
  compute_minp: true

# Output settings
output:
  dir: "./outputs"
  save_features: false

# Misc
seed: 42
device: "cuda"
